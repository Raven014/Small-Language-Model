{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a08bd747-8314-4b29-b2a8-a15efcef226c",
   "metadata": {},
   "source": [
    "# Small Language Model (SLM) for Book-Based Question Answering\n",
    "\n",
    "## 1. Introduction\n",
    "This project implements a Small Language Model (SLM) designed to answer questions based on the content of a given book. It uses a combination of **text retrieval** and **sequence-to-sequence learning** to extract relevant information and generate meaningful responses.\n",
    "\n",
    "## 2. Approach\n",
    "My approach follows a **Retrieval-Augmented Generation (RAG)** framework:\n",
    "1. **Text Preprocessing** - The book text is split into manageable chunks.\n",
    "2. **Embedding & Retrieval** - We encode these chunks using a sentence transformer and store them in a FAISS index for efficient retrieval.\n",
    "3. **Answer Generation** - Using a T5 model, we generate answers based on the retrieved relevant text.\n",
    "\n",
    "## 3. Model Architecture\n",
    "- **Sentence Transformer** (all-MiniLM-L6-v2): Used for encoding book text into dense vector representations for retrieval.\n",
    "- **FAISS**: A fast similarity search library to retrieve relevant text.\n",
    "- **T5-Small**: A transformer-based sequence-to-sequence model that generates answers based on retrieved context.\n",
    "\n",
    "## 4. Preprocessing Techniques\n",
    "1. **Text Tokenization & Chunking**\n",
    "   - The book text is split into smaller sections using the `nltk.sent_tokenize` method.\n",
    "   - Each chunk is stored as a potential context for answering queries.\n",
    "2. **Embedding Generation**\n",
    "   - Each text chunk is passed through `all-MiniLM-L6-v2`, and the embeddings are stored in a FAISS index for retrieval.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae48e41-7407-438c-ba2c-b9bf1a11ddeb",
   "metadata": {},
   "source": [
    "## 5. Code Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e1f947-f56f-4c79-80e0-8c313a6a8955",
   "metadata": {},
   "source": [
    "### **1. Import necessary libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bf3c1f0-b457-4a36-bcc2-49fa53dd408c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, AutoModel\n",
    "import faiss\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68cd4369-e2af-4e53-afeb-70dfe0518835",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rajee\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure nltk dependencies are available\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74cffb6-4d6c-4508-8212-b417f2564adc",
   "metadata": {},
   "source": [
    "### **2. Loading Pre-trained Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fface18-93c4-48d2-a2c1-bddc7558a69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Pre-trained Tokenizer and Model (DistilBERT for embeddings, T5 for generation)\n",
    "retrieval_model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\", trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\", trust_remote_code=True)\n",
    "generation_model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\", trust_remote_code=True)\n",
    "generation_tokenizer = AutoTokenizer.from_pretrained(\"t5-small\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e63f2d6-9528-49ca-957b-bc186afc5ada",
   "metadata": {},
   "source": [
    "- **Sentence Transformer**: Encodes book text.\n",
    "- **T5-Small**: Generates answers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403fc7b1-cb75-4deb-b0e6-35f845a60fda",
   "metadata": {},
   "source": [
    "### **3. Chunking the Book Text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cddc722-4f2a-4cce-8779-7442e7f8dc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split book text into chunks\n",
    "def chunk_text(text, chunk_size=200):\n",
    "    sentences = sent_tokenize(text)\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        current_length += len(sentence.split())\n",
    "        current_chunk.append(sentence)\n",
    "        if current_length >= chunk_size:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = []\n",
    "            current_length = 0\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ed2cb2-271a-4104-9443-946d51490f8f",
   "metadata": {},
   "source": [
    "- Splits long text into manageable chunks (200 words per chunk)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26421ebe-cd2f-4df2-a71a-ce30847a791e",
   "metadata": {},
   "source": [
    "### **4. Encoding Text into Embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d78dfaab-42c1-4bff-bee7-0696138a5d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to encode text into embeddings\n",
    "def encode_text(text_list, model, tokenizer):\n",
    "    inputs = tokenizer(text_list, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        if hasattr(outputs, 'last_hidden_state'):\n",
    "            embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "        else:\n",
    "            embeddings = outputs.pooler_output\n",
    "    return embeddings.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22224d34-22c2-4042-aa0d-d5997c5523bb",
   "metadata": {},
   "source": [
    "- Converts book chunks into numerical embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8f2822-79a2-47cc-839a-c9beedc0110c",
   "metadata": {},
   "source": [
    "### **5. Building FAISS Index**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "662c226a-9fa7-401e-b431-4bb524b234c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to build FAISS index\n",
    "def build_faiss_index(chunks, model, tokenizer):\n",
    "    embeddings = encode_text(chunks, model, tokenizer)\n",
    "    index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "    index.add(embeddings)\n",
    "    return index, embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120ddb39-ec0b-4835-b40c-bfdceefa3a93",
   "metadata": {},
   "source": [
    "- Stores embeddings for efficient retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e81570b-0e3c-4dd8-8437-af20d6436171",
   "metadata": {},
   "source": [
    "### **6. Retrieving Relevant Text for a Query**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6efbb81-12bc-4be7-8f28-ba5e929bde30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to retrieve relevant text\n",
    "def retrieve_text(query, chunks, index, model, tokenizer, top_k=3):\n",
    "    query_embedding = encode_text([query], model, tokenizer)\n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "    return [chunks[i] for i in indices[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272c4783-034b-4485-99e5-f3f07d0d3eb4",
   "metadata": {},
   "source": [
    "- Retrieves the most relevant book chunks based on the user's question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0819e1f-deae-434e-a8c6-3eb6db00baab",
   "metadata": {},
   "source": [
    "### **7. Answer Generation using T5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c8b3ae3-99eb-4875-ab00-ffe054391d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate answers using T5 model\n",
    "def generate_answer(context, question, model, tokenizer):\n",
    "    input_text = f\"question: {question} context: {context}\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True)\n",
    "    outputs = model.generate(**inputs)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfa7ca2-579f-4389-a22c-0f86ccf84211",
   "metadata": {},
   "source": [
    "- Uses the T5 model to generate a natural language response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b9c807-7cd2-4926-bd8e-197ad8afcd4e",
   "metadata": {},
   "source": [
    "### **8. Main function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9bed4dc-cf20-4960-9447-3d2062527530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to load book text, process it, and answer questions\n",
    "def answer_question(book_text, question):\n",
    "    chunks = chunk_text(book_text)\n",
    "    index, _ = build_faiss_index(chunks, retrieval_model, tokenizer)\n",
    "    relevant_text = retrieve_text(question, chunks, index, retrieval_model, tokenizer)\n",
    "    combined_context = \" \".join(relevant_text)\n",
    "    return generate_answer(combined_context, question, generation_model, generation_tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ff69c7-7acf-4489-ad5a-878835ce6282",
   "metadata": {},
   "source": [
    "### **9. Interactive User Input**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767bdf61-c997-4bd1-bc5b-3968b4be7308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the book text:  n a distant kingdom surrounded by towering mountains and vast rivers, there existed an ancient city known for its wisdom and prosperity. The city was home to scholars, inventors, and artists who dedicated their lives to knowledge and creativity. One day, a young boy named Elias discovered an old manuscript hidden deep within the grand library. The manuscript spoke of a legendary artifact—an enchanted compass said to guide its bearer to the lost city of Eldoria, where the secrets of the universe were kept. Driven by curiosity and a thirst for adventure, Elias set out on a journey filled with challenges, encountering mystical creatures, deciphering ancient riddles, and braving perilous landscapes. Along the way, he met companions who shared his quest, each possessing unique skills that contributed to the adventure. As he drew closer to Eldoria, he realized that the journey itself was a test of wisdom and courage. The true treasure was not the secrets hidden in Eldoria, but the knowledge and strength he had gained throughout his journey. Upon reaching the city, Elias was faced with a final challenge: to prove himself worthy of the wisdom it held. With determination and insight, he overcame the trial, unlocking the hidden truths of the universe and earning his place among the scholars of old.\n",
      "Ask a question (or type 'exit' to quit):  What was Elias searching for?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: ancient manuscript\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ask a question (or type 'exit' to quit):  What challenges did he face?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: encountering mystical creatures, deciphering ancient riddles, and braving peril\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ask a question (or type 'exit' to quit):  Who did Elias meet on his journey?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: companions\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ask a question (or type 'exit' to quit):  What was the real treasure of the journey?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: knowledge and strength\n"
     ]
    }
   ],
   "source": [
    "# Interactive user input\n",
    "if __name__ == \"__main__\":\n",
    "    book_text = input(\"Enter the book text: \")\n",
    "    while True:\n",
    "        question = input(\"Ask a question (or type 'exit' to quit): \")\n",
    "        if question.lower() == 'exit':\n",
    "            break\n",
    "        answer = answer_question(book_text, question)\n",
    "        print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c832e965-b216-47a6-a983-8b645cbc921a",
   "metadata": {},
   "source": [
    "## 10. Observations & Learnings\n",
    "### **Key Takeaways:**\n",
    "- **FAISS** enables fast retrieval of relevant text chunks.\n",
    "- **T5-Small** is efficient for text generation but may require fine-tuning for better domain adaptation.\n",
    "- **Chunking Strategy** affects retrieval quality; larger chunks may include more relevant context.\n",
    "- **Interactive Design** in Jupyter enhances usability.\n",
    "\n",
    "### **Future Improvements:**\n",
    "- Fine-tune `T5-Small` on a custom dataset for improved accuracy.\n",
    "- Experiment with larger models like `T5-Base` for better answer quality.\n",
    "- Implement **metadata filtering** to improve retrieval precision.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c00ccf-90dd-4487-b5d8-c93f469f8066",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
